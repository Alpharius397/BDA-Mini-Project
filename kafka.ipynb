{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install kafka-python pyspark==3.3.2 findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "from kafka.consumer.fetcher import ConsumerRecord\n",
    "import json\n",
    "import asyncio\n",
    "import os\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "import findspark\n",
    "from pyspark.sql.functions import from_csv, col, avg, round as _round, countDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init(os.environ[\"SPARK_HOME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC = \"KafkaMusicStream\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = KafkaConsumer(TOPIC, bootstrap_servers=\"localhost:9092\", consumer_timeout_ms=5000)\n",
    "producer = KafkaProducer(bootstrap_servers=\"localhost:9092\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_consumer(consumer: KafkaConsumer):\n",
    "    for msg in consumer:\n",
    "        await asyncio.sleep(1)\n",
    "        assert isinstance(msg, ConsumerRecord), \"Invalid Type\"\n",
    "        print(json.loads(msg.value))\n",
    "        \n",
    "def run_producer(producer: KafkaProducer):\n",
    "    with open(\"data/SpotifyFeatures.csv\") as f:\n",
    "        f.readline() # first line\n",
    "        \n",
    "        while((line := f.readline()) != \"\"):\n",
    "            producer.send(TOPIC, line.encode())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/11/06 13:36:01 WARN Utils: Your hostname, LAPTOP-IVV27U6L resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/11/06 13:36:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/omnissiah/SPARK/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/omnissiah/.ivy2/cache\n",
      "The jars for the packages stored in: /home/omnissiah/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-81ea3652-5d13-4f42-8bc8-474fe682347d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 687ms :: artifacts dl 32ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-81ea3652-5d13-4f42-8bc8-474fe682347d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/15ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/11/06 13:36:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark started: <pyspark.sql.session.SparkSession object at 0x749e3f36cc50>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .master(\"local[*]\")\n",
    "         .appName(\"KafkaSqlStream\")\n",
    "         .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\")\n",
    "         .config(\"spark.sql.warehouse.dir\", \"spark-warehouse\")  \n",
    "         .config(\"spark.driver.memory\", \"4g\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate())\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"Spark started:\", spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from Kafka\n",
    "kafka_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", TOPIC) \\\n",
    "    .load()\n",
    "    \n",
    "schema = StructType([\n",
    "    StructField(\"genre\", StringType(), True),\n",
    "    StructField(\"artist_name\", StringType(), True),\n",
    "    StructField(\"track_name\", StringType(), True),\n",
    "    StructField(\"track_id\", StringType(), True),\n",
    "    StructField(\"popularity\", IntegerType(), True),\n",
    "    StructField(\"acousticness\", DoubleType(), True),\n",
    "    StructField(\"danceability\", DoubleType(), True),\n",
    "    StructField(\"duration_ms\", IntegerType(), True),\n",
    "    StructField(\"energy\", DoubleType(), True),\n",
    "    StructField(\"instrumentalness\", DoubleType(), True),\n",
    "    StructField(\"key\", StringType(), True),\n",
    "    StructField(\"liveness\", DoubleType(), True),\n",
    "    StructField(\"loudness\", DoubleType(), True),\n",
    "    StructField(\"mode\", StringType(), True),\n",
    "    StructField(\"speechiness\", DoubleType(), True),\n",
    "    StructField(\"tempo\", DoubleType(), True),\n",
    "    StructField(\"time_signature\", StringType(), True),\n",
    "    StructField(\"valence\", DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/11/06 13:36:11 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-d4685111-a4c0-4427-865a-933f4f154757. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/11/06 13:36:11 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----+--------------+-----+\n",
      "|genre|avg_popularity|count|\n",
      "+-----+--------------+-----+\n",
      "+-----+--------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----------+--------------+-----+\n",
      "|genre      |avg_popularity|count|\n",
      "+-----------+--------------+-----+\n",
      "|Alternative|46.74         |1600 |\n",
      "+-----------+--------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----------+------------------+-----+\n",
      "|genre      |avg_popularity    |count|\n",
      "+-----------+------------------+-----+\n",
      "|Alternative|46.599444444444444|1800 |\n",
      "+-----------+------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-----------+-----------------+-----+\n",
      "|genre      |avg_popularity   |count|\n",
      "+-----------+-----------------+-----+\n",
      "|Alternative|46.09291666666667|2400 |\n",
      "+-----------+-----------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-----------+-----------------+-----+\n",
      "|genre      |avg_popularity   |count|\n",
      "+-----------+-----------------+-----+\n",
      "|Alternative|45.52199874292898|3182 |\n",
      "|Dance      |80.83333333333333|18   |\n",
      "+-----------+-----------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+-----------+-----------------+-----+\n",
      "|genre      |avg_popularity   |count|\n",
      "+-----------+-----------------+-----+\n",
      "|Alternative|45.52199874292898|3182 |\n",
      "|Dance      |71.29095354523227|818  |\n",
      "+-----------+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    csv_df = kafka_df.selectExpr(\"CAST(value AS STRING) as csv_value\")\n",
    "    parsed_df = csv_df.select(from_csv(\"csv_value\", schema.simpleString()).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "    parsed_df.createOrReplaceTempView(\"music_stream\")\n",
    "\n",
    "    sql_df = spark.sql(\"\"\"\n",
    "        SELECT genre, AVG(popularity) AS avg_popularity, COUNT(*) AS count\n",
    "        FROM music_stream\n",
    "        GROUP BY genre\n",
    "    \"\"\")\n",
    "\n",
    "    query = sql_df.writeStream \\\n",
    "        .outputMode(\"complete\") \\\n",
    "        .format(\"console\") \\\n",
    "        .option(\"truncate\", False) \\\n",
    "        .start()\n",
    "\n",
    "    query.awaitTermination(30)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/11/06 13:36:41 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-81f276ba-ed5d-4659-95ec-78755ec1bb59. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/11/06 13:36:41 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/11/06 13:36:41 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-83291732-8643-47ba-9848-faccc939129d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/11/06 13:36:41 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/11/06 13:36:41 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-cd433307-1ee6-4942-a17a-914140430453. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/11/06 13:36:41 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/11/06 13:36:42 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-b4c56d8a-cb21-48f7-adf7-6aa824b39362. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/11/06 13:36:42 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/11/06 13:36:42 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-80b8fb62-789b-4aa0-b700-c46804008335. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/11/06 13:36:42 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/11/06 13:36:42 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-75706174-6387-4080-a7bc-48dbb0099871. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/11/06 13:36:42 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/11/06 13:36:45 ERROR MicroBatchExecution: Query [id = e9c233d5-5476-4b08-9e13-b93dbe92250b, runId = e7bf861b-b4e7-4480-9d33-b487a0569f26] terminated with error\n",
      "org.apache.spark.SparkException: The Spark SQL phase planning failed with an internal error. Please, fill a bug report in, and provide the full stack trace.\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.toInternalError(QueryExecution.scala:500)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:512)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:145)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:138)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:158)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:158)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:151)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$15(MicroBatchExecution.scala:656)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:646)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.getSortedExecutorList(KafkaOffsetReaderConsumer.scala:483)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.getOffsetRangesFromResolvedOffsets(KafkaOffsetReaderConsumer.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.planInputPartitions(KafkaMicroBatchStream.scala:201)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.inputPartitions$lzycompute(MicroBatchScanExec.scala:45)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.inputPartitions(MicroBatchScanExec.scala:45)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.supportsColumnar(DataSourceV2ScanExecBase.scala:142)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.supportsColumnar$(DataSourceV2ScanExecBase.scala:141)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.supportsColumnar(MicroBatchScanExec.scala:29)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.apply(DataSourceV2Strategy.scala:153)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:459)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$1(QueryExecution.scala:145)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n",
      "\t... 32 more\n",
      "25/11/06 13:36:45 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@243a5095[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@534f9fbf[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@468fe42a]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@5a9e580d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "25/11/06 13:36:45 ERROR MicroBatchExecution: Query [id = 89e40ca8-fd1b-4b4e-aea4-1f80401ee054, runId = 9ae54a5b-ffe2-4a4b-919f-f2a711d94fa0] terminated with error\n",
      "org.apache.spark.SparkException: The Spark SQL phase planning failed with an internal error. Please, fill a bug report in, and provide the full stack trace.\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.toInternalError(QueryExecution.scala:500)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:512)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:145)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:138)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:158)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:158)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:151)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$15(MicroBatchExecution.scala:656)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:646)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.getSortedExecutorList(KafkaOffsetReaderConsumer.scala:483)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.getOffsetRangesFromResolvedOffsets(KafkaOffsetReaderConsumer.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.planInputPartitions(KafkaMicroBatchStream.scala:201)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.inputPartitions$lzycompute(MicroBatchScanExec.scala:45)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.inputPartitions(MicroBatchScanExec.scala:45)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.supportsColumnar(DataSourceV2ScanExecBase.scala:142)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.supportsColumnar$(DataSourceV2ScanExecBase.scala:141)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.supportsColumnar(MicroBatchScanExec.scala:29)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.apply(DataSourceV2Strategy.scala:153)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:459)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$1(QueryExecution.scala:145)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n",
      "\t... 32 more\n",
      "25/11/06 13:36:45 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@2babb9b[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@3bff215[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@1d27e97d]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@5a9e580d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "25/11/06 13:36:45 ERROR MicroBatchExecution: Query [id = baf03d7b-834c-445e-bf8e-8a23da77a3b3, runId = 8c24e779-3bef-4bbd-9e28-c55b437b88b5] terminated with error\n",
      "org.apache.spark.SparkException: The Spark SQL phase planning failed with an internal error. Please, fill a bug report in, and provide the full stack trace.\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.toInternalError(QueryExecution.scala:500)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:512)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:145)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:138)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:158)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:158)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:151)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$15(MicroBatchExecution.scala:656)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:646)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.getSortedExecutorList(KafkaOffsetReaderConsumer.scala:483)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.getOffsetRangesFromResolvedOffsets(KafkaOffsetReaderConsumer.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.planInputPartitions(KafkaMicroBatchStream.scala:201)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.inputPartitions$lzycompute(MicroBatchScanExec.scala:45)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.inputPartitions(MicroBatchScanExec.scala:45)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.supportsColumnar(DataSourceV2ScanExecBase.scala:142)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.supportsColumnar$(DataSourceV2ScanExecBase.scala:141)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.supportsColumnar(MicroBatchScanExec.scala:29)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.apply(DataSourceV2Strategy.scala:153)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:459)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$1(QueryExecution.scala:145)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n",
      "\t... 32 more\n",
      "25/11/06 13:36:45 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@6f04ded9[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@60b57353[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@638f0ee0]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@5a9e580d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"stream execution thread for [id = e9c233d5-5476-4b08-9e13-b93dbe92250b, runId = e7bf861b-b4e7-4480-9d33-b487a0569f26]\" org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef.deactivateInstances(StateStoreCoordinator.scala:119)\n",
      "\tat org.apache.spark.sql.streaming.StreamingQueryManager.notifyQueryTermination(StreamingQueryManager.scala:406)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$3(StreamExecution.scala:357)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:338)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n",
      "Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:554)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:558)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)\n",
      "\t... 8 more\n",
      "Exception in thread \"stream execution thread for [id = 89e40ca8-fd1b-4b4e-aea4-1f80401ee054, runId = 9ae54a5b-ffe2-4a4b-919f-f2a711d94fa0]\" org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef.deactivateInstances(StateStoreCoordinator.scala:119)\n",
      "\tat org.apache.spark.sql.streaming.StreamingQueryManager.notifyQueryTermination(StreamingQueryManager.scala:406)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$3(StreamExecution.scala:357)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:338)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n",
      "Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:554)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:558)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)\n",
      "\t... 8 more\n",
      "Exception in thread \"stream execution thread for [id = baf03d7b-834c-445e-bf8e-8a23da77a3b3, runId = 8c24e779-3bef-4bbd-9e28-c55b437b88b5]\" org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef.deactivateInstances(StateStoreCoordinator.scala:119)\n",
      "\tat org.apache.spark.sql.streaming.StreamingQueryManager.notifyQueryTermination(StreamingQueryManager.scala:406)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$3(StreamExecution.scala:357)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:338)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n",
      "Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:554)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:558)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/11/06 13:36:45 ERROR MicroBatchExecution: Query [id = 1911559c-fa4d-489a-9a1b-273721861905, runId = 1c750ad8-f972-43fb-8ad6-90c58d85e3e9] terminated with error\n",
      "org.apache.spark.SparkException: The Spark SQL phase planning failed with an internal error. Please, fill a bug report in, and provide the full stack trace.\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.toInternalError(QueryExecution.scala:500)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:512)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:145)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:138)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:158)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:158)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:151)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$15(MicroBatchExecution.scala:656)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:646)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.getSortedExecutorList(KafkaOffsetReaderConsumer.scala:483)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.getOffsetRangesFromResolvedOffsets(KafkaOffsetReaderConsumer.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.planInputPartitions(KafkaMicroBatchStream.scala:201)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.inputPartitions$lzycompute(MicroBatchScanExec.scala:45)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.inputPartitions(MicroBatchScanExec.scala:45)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.supportsColumnar(DataSourceV2ScanExecBase.scala:142)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.supportsColumnar$(DataSourceV2ScanExecBase.scala:141)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.supportsColumnar(MicroBatchScanExec.scala:29)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.apply(DataSourceV2Strategy.scala:153)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:459)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$1(QueryExecution.scala:145)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n",
      "\t... 32 more\n",
      "25/11/06 13:36:45 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@249a48b3[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@243a9587[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@63d404da]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@5a9e580d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "25/11/06 13:36:45 ERROR MicroBatchExecution: Query [id = 904114e8-df60-4f1e-960b-ec1cff38edaf, runId = cd6ee6de-2877-4b8b-8870-3c7cf84d13c8] terminated with error\n",
      "org.apache.spark.SparkException: The Spark SQL phase planning failed with an internal error. Please, fill a bug report in, and provide the full stack trace.\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.toInternalError(QueryExecution.scala:500)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:512)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:145)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:138)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:158)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:158)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:151)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$15(MicroBatchExecution.scala:656)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:646)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.getSortedExecutorList(KafkaOffsetReaderConsumer.scala:483)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.getOffsetRangesFromResolvedOffsets(KafkaOffsetReaderConsumer.scala:538)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.planInputPartitions(KafkaMicroBatchStream.scala:201)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.inputPartitions$lzycompute(MicroBatchScanExec.scala:45)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.inputPartitions(MicroBatchScanExec.scala:45)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.supportsColumnar(DataSourceV2ScanExecBase.scala:142)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.supportsColumnar$(DataSourceV2ScanExecBase.scala:141)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.supportsColumnar(MicroBatchScanExec.scala:29)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.apply(DataSourceV2Strategy.scala:153)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:69)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:459)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$1(QueryExecution.scala:145)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n",
      "\t... 32 more\n",
      "25/11/06 13:36:45 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@708e8316[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@1fed7f3f[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@44ab91fd]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@5a9e580d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"stream execution thread for [id = 1911559c-fa4d-489a-9a1b-273721861905, runId = 1c750ad8-f972-43fb-8ad6-90c58d85e3e9]\" org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef.deactivateInstances(StateStoreCoordinator.scala:119)\n",
      "\tat org.apache.spark.sql.streaming.StreamingQueryManager.notifyQueryTermination(StreamingQueryManager.scala:406)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$3(StreamExecution.scala:357)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:338)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n",
      "Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:554)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:558)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)\n",
      "\t... 8 more\n",
      "Exception in thread \"stream execution thread for [id = 904114e8-df60-4f1e-960b-ec1cff38edaf, runId = cd6ee6de-2877-4b8b-8870-3c7cf84d13c8]\" org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef.deactivateInstances(StateStoreCoordinator.scala:119)\n",
      "\tat org.apache.spark.sql.streaming.StreamingQueryManager.notifyQueryTermination(StreamingQueryManager.scala:406)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$3(StreamExecution.scala:357)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:338)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n",
      "Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:554)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:558)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)\n",
      "\t... 8 more\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/11/06 13:37:17 WARN StateStore: Error running maintenance thread\n",
      "java.lang.IllegalStateException: SparkEnv not active, cannot do maintenance on StateStores\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.doMaintenance(StateStore.scala:596)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.$anonfun$startMaintenanceIfNeeded$1(StateStore.scala:582)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$MaintenanceTask$$anon$1.run(StateStore.scala:442)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/11/06 13:38:14 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:14 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:14 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:14 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:15 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:16 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:17 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:18 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:19 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:20 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:21 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:22 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:23 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:24 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:25 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:26 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:27 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:28 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:29 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:30 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:31 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:32 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:34 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:35 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:36 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:37 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:38 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:39 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:40 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:41 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:42 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:43 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:44 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:45 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:46 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:47 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:48 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:49 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:50 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:51 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:52 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:53 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:54 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:55 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:56 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:57 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:38:58 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:39:00 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:39:00 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:39:02 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:39:03 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:39:04 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:39:05 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:39:06 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:39:07 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:39:08 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:39:09 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:39:11 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:39:11 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:39:12 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:39:13 WARN KafkaOffsetReaderConsumer: Error in attempt 1 getting Kafka offsets: \n",
      "org.apache.kafka.common.errors.TimeoutException: Timeout of 60000ms expired before the position for partition KafkaMusicStream-0 could be determined\n",
      "25/11/06 13:39:14 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:39:14 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0-1, groupId=spark-kafka-source-168b7a73-d455-44c1-816c-8d8250196862--1150029504-driver-0] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "25/11/06 13:39:14 WARN KafkaOffsetReaderConsumer: Error in attempt 2 getting Kafka offsets: \n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.kafka010.KafkaConfigUpdater.setAuthenticationConfigIfNeeded(KafkaConfigUpdater.scala:60)\n",
      "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.setAuthenticationConfigIfNeeded(ConsumerStrategy.scala:61)\n",
      "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.setAuthenticationConfigIfNeeded$(ConsumerStrategy.scala:60)\n",
      "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.setAuthenticationConfigIfNeeded(ConsumerStrategy.scala:102)\n",
      "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.createConsumer(ConsumerStrategy.scala:106)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.consumer(KafkaOffsetReaderConsumer.scala:83)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReaderConsumer.scala:548)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReaderConsumer.scala:593)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.withRetriesWithoutInterrupt(KafkaOffsetReaderConsumer.scala:592)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReaderConsumer.scala:546)\n",
      "\tat org.apache.spark.util.UninterruptibleThreadRunner.runUninterruptibly(UninterruptibleThreadRunner.scala:48)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.partitionsAssignedToConsumer(KafkaOffsetReaderConsumer.scala:546)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.fetchLatestOffsets(KafkaOffsetReaderConsumer.scala:344)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:130)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:449)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:448)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:437)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:692)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:433)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:238)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n",
      "25/11/06 13:39:14 WARN KafkaOffsetReaderConsumer: Error in attempt 3 getting Kafka offsets: \n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.kafka010.KafkaConfigUpdater.setAuthenticationConfigIfNeeded(KafkaConfigUpdater.scala:60)\n",
      "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.setAuthenticationConfigIfNeeded(ConsumerStrategy.scala:61)\n",
      "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.setAuthenticationConfigIfNeeded$(ConsumerStrategy.scala:60)\n",
      "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.setAuthenticationConfigIfNeeded(ConsumerStrategy.scala:102)\n",
      "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.createConsumer(ConsumerStrategy.scala:106)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.consumer(KafkaOffsetReaderConsumer.scala:83)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReaderConsumer.scala:548)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReaderConsumer.scala:593)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.withRetriesWithoutInterrupt(KafkaOffsetReaderConsumer.scala:592)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReaderConsumer.scala:546)\n",
      "\tat org.apache.spark.util.UninterruptibleThreadRunner.runUninterruptibly(UninterruptibleThreadRunner.scala:48)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.partitionsAssignedToConsumer(KafkaOffsetReaderConsumer.scala:546)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.fetchLatestOffsets(KafkaOffsetReaderConsumer.scala:344)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:130)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:449)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:448)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:437)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:692)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:433)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:238)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n",
      "25/11/06 13:39:15 ERROR MicroBatchExecution: Query [id = 3de4960a-4cbb-47a6-8a2a-2a9455012ee4, runId = ce36e69e-6378-49b8-a2b5-acfdbfe92d1d] terminated with error\n",
      "org.apache.spark.SparkException: Execution of the stream null failed. Please, fill a bug report in, and provide the full stack trace.\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.toInternalError(QueryExecution.scala:500)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:324)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\tat org.apache.spark.kafka010.KafkaConfigUpdater.setAuthenticationConfigIfNeeded(KafkaConfigUpdater.scala:60)\n",
      "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.setAuthenticationConfigIfNeeded(ConsumerStrategy.scala:61)\n",
      "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.setAuthenticationConfigIfNeeded$(ConsumerStrategy.scala:60)\n",
      "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.setAuthenticationConfigIfNeeded(ConsumerStrategy.scala:102)\n",
      "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.createConsumer(ConsumerStrategy.scala:106)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.consumer(KafkaOffsetReaderConsumer.scala:83)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReaderConsumer.scala:548)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReaderConsumer.scala:593)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.withRetriesWithoutInterrupt(KafkaOffsetReaderConsumer.scala:592)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReaderConsumer.scala:546)\n",
      "\tat org.apache.spark.util.UninterruptibleThreadRunner.runUninterruptibly(UninterruptibleThreadRunner.scala:48)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.partitionsAssignedToConsumer(KafkaOffsetReaderConsumer.scala:546)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.fetchLatestOffsets(KafkaOffsetReaderConsumer.scala:344)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:130)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:449)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:448)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:437)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:692)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:433)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:238)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\t... 1 more\n",
      "25/11/06 13:39:15 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@723d88ea[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@512696e4[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@1c5e91d8]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@5a9e580d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"stream execution thread for [id = 3de4960a-4cbb-47a6-8a2a-2a9455012ee4, runId = ce36e69e-6378-49b8-a2b5-acfdbfe92d1d]\" org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef.deactivateInstances(StateStoreCoordinator.scala:119)\n",
      "\tat org.apache.spark.sql.streaming.StreamingQueryManager.notifyQueryTermination(StreamingQueryManager.scala:406)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$3(StreamExecution.scala:357)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:338)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n",
      "Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:554)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:558)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)\n",
      "\t... 8 more\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "\n",
    "    with spark as sp:\n",
    "        top_genres = (\n",
    "            parsed_df.groupBy(\"genre\")\n",
    "                    .agg(_round(avg(\"popularity\"), 2).alias(\"avg_popularity\"))\n",
    "                    .orderBy(col(\"avg_popularity\").desc())\n",
    "        )\n",
    "\n",
    "        query1 = (\n",
    "            top_genres.writeStream\n",
    "                    .outputMode(\"complete\")\n",
    "                    .format(\"console\")\n",
    "                    .option(\"truncate\", False)\n",
    "                    .start()\n",
    "        )\n",
    "        \n",
    "        energy_dance = (\n",
    "            parsed_df.groupBy(\"genre\")\n",
    "                    .agg(\n",
    "                        _round(avg(\"energy\"), 3).alias(\"avg_energy\"),\n",
    "                        _round(avg(\"danceability\"), 3).alias(\"avg_danceability\")\n",
    "                    )\n",
    "        )\n",
    "        \n",
    "        query2 = (\n",
    "            energy_dance.writeStream\n",
    "                        .outputMode(\"complete\")\n",
    "                        .format(\"console\")\n",
    "                        .option(\"truncate\", False)\n",
    "                        .start()\n",
    "        )\n",
    "        \n",
    "        top_artists = (\n",
    "            parsed_df.groupBy(\"artist_name\")\n",
    "                    .count()\n",
    "                    .withColumnRenamed(\"count\", \"total_songs\")\n",
    "                    .orderBy(col(\"total_songs\").desc())\n",
    "        )\n",
    "\n",
    "        query3 = (\n",
    "            top_artists.writeStream\n",
    "                    .outputMode(\"complete\")\n",
    "                    .format(\"console\")\n",
    "                    .option(\"truncate\", False)\n",
    "                    .start()\n",
    "        )\n",
    "\n",
    "        # --- New Queries ---\n",
    "\n",
    "        # 4. Average valence and energy per genre (mood analysis)\n",
    "        mood_stats = (\n",
    "            parsed_df.groupBy(\"genre\")\n",
    "                    .agg(\n",
    "                        _round(avg(\"valence\"), 3).alias(\"avg_valence\"),\n",
    "                        _round(avg(\"energy\"), 3).alias(\"avg_energy\")\n",
    "                    )\n",
    "                    .orderBy(col(\"avg_valence\").desc())\n",
    "        )\n",
    "\n",
    "        query4 = (\n",
    "            mood_stats.writeStream\n",
    "                    .outputMode(\"complete\")\n",
    "                    .format(\"console\")\n",
    "                    .option(\"truncate\", False)\n",
    "                    .start()\n",
    "        )\n",
    "\n",
    "        # 5. Average loudness and acousticness per genre\n",
    "        sound_profile = (\n",
    "            parsed_df.groupBy(\"genre\")\n",
    "                    .agg(\n",
    "                        _round(avg(\"loudness\"), 2).alias(\"avg_loudness\"),\n",
    "                        _round(avg(\"acousticness\"), 3).alias(\"avg_acousticness\")\n",
    "                    )\n",
    "                    .orderBy(col(\"avg_loudness\").desc())\n",
    "        )\n",
    "\n",
    "        query5 = (\n",
    "            sound_profile.writeStream\n",
    "                    .outputMode(\"complete\")\n",
    "                    .format(\"console\")\n",
    "                    .option(\"truncate\", False)\n",
    "                    .start()\n",
    "        )\n",
    "\n",
    "        # 6. Average tempo and time signature distribution\n",
    "        tempo_stats = (\n",
    "            parsed_df.groupBy(\"genre\")\n",
    "                    .agg(\n",
    "                        _round(avg(\"tempo\"), 2).alias(\"avg_tempo\"),\n",
    "                        countDistinct(\"time_signature\").alias(\"unique_time_sigs\")\n",
    "                    )\n",
    "                    .orderBy(col(\"avg_tempo\").desc())\n",
    "        )\n",
    "\n",
    "        query6 = (\n",
    "            tempo_stats.writeStream\n",
    "                    .outputMode(\"complete\")\n",
    "                    .format(\"console\")\n",
    "                    .option(\"truncate\", False)\n",
    "                    .start()\n",
    "        )\n",
    "\n",
    "        # 7. Top tracks by popularity\n",
    "        top_tracks = (\n",
    "            parsed_df.select(\"track_name\", \"artist_name\", \"popularity\")\n",
    "                    .orderBy(col(\"popularity\").desc())\n",
    "                    .limit(10)\n",
    "        )\n",
    "\n",
    "        query7 = (\n",
    "            top_tracks.writeStream\n",
    "                    .outputMode(\"complete\")\n",
    "                    .format(\"console\")\n",
    "                    .option(\"truncate\", False)\n",
    "                    .start()\n",
    "        )\n",
    "\n",
    "        sp.streams.awaitAnyTermination()\n",
    "\n",
    "except:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
